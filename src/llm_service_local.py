"""
Local LLM Servisi
GGUF ve HuggingFace model desteƒüi
A100 GPU optimizasyonlarƒ± dahil
"""

import logging
import os
import torch
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# GGUF desteƒüi i√ßin llama-cpp-python import
try:
    from llama_cpp import Llama
    GGUF_AVAILABLE = True
    logger.info("‚úÖ llama-cpp-python available for GGUF models")
except ImportError:
    GGUF_AVAILABLE = False
    logger.warning("‚ö†Ô∏è llama-cpp-python not available for GGUF models")

# HuggingFace transformers desteƒüi
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    HF_AVAILABLE = True
    logger.info("‚úÖ HuggingFace transformers available")
except ImportError:
    HF_AVAILABLE = False
    logger.warning("‚ö†Ô∏è HuggingFace transformers not available")

@dataclass
class ChatMessage:
    """Chat mesajƒ± container'ƒ±"""
    role: str  # 'user', 'assistant', 'system'
    content: str
    timestamp: datetime

# √ñnerilen modeller
RECOMMENDED_MODELS = {
    "mistral_7b_gguf": {
        "name": "Mistral 7B GGUF",
        "type": "gguf",
        "description": "Mistral 7B Instruct - GGUF format (√ñnerilen)",
        "memory_requirement": "8GB",
        "performance": "Y√ºksek"
    },
    "llama_3_1_8b": {
        "name": "Llama 3.1 8B",
        "type": "huggingface",
        "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "description": "Meta Llama 3.1 8B Instruct",
        "memory_requirement": "16GB",
        "performance": "√áok Y√ºksek"
    },
    "custom_drive_model": {
        "name": "Custom Drive Model",
        "type": "custom",
        "description": "Google Drive'daki √∂zel model",
        "memory_requirement": "Deƒüi≈üken",
        "performance": "Deƒüi≈üken"
    }
}

class GGUFModelService:
    """
    GGUF modelleri i√ßin servis sƒ±nƒ±fƒ±
    A100 GPU optimizasyonlarƒ± dahil
    """
    
    def __init__(self, model_path: str):
        """
        Args:
            model_path: GGUF model dosyasƒ±nƒ±n yolu
        """
        self.model_path = model_path
        self.model_name = os.path.basename(model_path)
        self.llm = None
        self._initialize_model()
    
    def _initialize_model(self):
        """GGUF modelini y√ºkle ve A100 i√ßin optimize et"""
        try:
            logger.info(f"üöÄ GGUF model y√ºkleniyor: {self.model_name}")
            
            # GPU ve sistem kontrol√º
            gpu_info = self._detect_gpu()
            logger.info(f"üîß GPU Info: {gpu_info}")
            
            # A100 optimizasyonlarƒ±
            model_params = self._get_optimized_params(gpu_info)
            logger.info(f"‚öôÔ∏è Model parametreleri: {model_params}")
            
            # Modeli y√ºkle
            self.llm = Llama(
                model_path=self.model_path,
                **model_params
            )
            
            logger.info("‚úÖ GGUF model ba≈üarƒ±yla y√ºklendi")
            
        except Exception as e:
            logger.error(f"‚ùå GGUF model y√ºklenemedi: {e}")
            self.llm = None
    
    def _detect_gpu(self) -> Dict:
        """GPU bilgilerini topla"""
        gpu_info = {
            "has_cuda": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_name": "",
            "gpu_memory": 0,
            "is_a100": False
        }
        
        if gpu_info["has_cuda"] and gpu_info["gpu_count"] > 0:
            try:
                gpu_info["gpu_name"] = torch.cuda.get_device_name(0)
                gpu_properties = torch.cuda.get_device_properties(0)
                gpu_info["gpu_memory"] = gpu_properties.total_memory / 1e9  # GB
                gpu_info["is_a100"] = "A100" in gpu_info["gpu_name"]
                
                logger.info(f"üéØ GPU: {gpu_info['gpu_name']}")
                logger.info(f"üíæ GPU Memory: {gpu_info['gpu_memory']:.1f}GB")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è GPU bilgisi alƒ±namadƒ±: {e}")
        
        return gpu_info
    
    def _get_optimized_params(self, gpu_info: Dict) -> Dict:
        """GPU'ya g√∂re optimize edilmi≈ü parametreleri d√∂nd√ºr"""
        params = {
            "verbose": True,
            "use_mmap": True,
            "use_mlock": False,
            "numa": False,
        }
        
        if gpu_info["has_cuda"]:
            if gpu_info["is_a100"]:
                # A100 i√ßin maksimum performans
                params.update({
                    "n_ctx": 8192,          # B√ºy√ºk context window
                    "n_batch": 4096,        # B√ºy√ºk batch size
                    "n_threads": 8,         # Daha fazla CPU thread
                    "n_gpu_layers": -1,     # T√ºm layer'larƒ± GPU'ya
                    "low_vram": False,      # A100'de VRAM bol
                    "f16_kv": True,         # Half precision
                    "logits_all": False,    # Memory optimization
                    "n_threads_batch": 8,   # Batch threading
                })
                logger.info("üéØ A100 optimization enabled")
                
            elif gpu_info["gpu_memory"] > 20:
                # Y√ºksek VRAM GPU'lar
                params.update({
                    "n_ctx": 6144,
                    "n_batch": 3072,
                    "n_threads": 6,
                    "n_gpu_layers": -1,
                    "low_vram": False,
                    "f16_kv": True,
                    "logits_all": False,
                    "n_threads_batch": 6,
                })
                logger.info("üöÄ High-end GPU optimization enabled")
                
            elif gpu_info["gpu_memory"] > 8:
                # Orta seviye GPU'lar
                params.update({
                    "n_ctx": 4096,
                    "n_batch": 2048,
                    "n_threads": 4,
                    "n_gpu_layers": 32,     # Kƒ±smi GPU offload
                    "low_vram": True,
                    "f16_kv": True,
                    "logits_all": False,
                    "n_threads_batch": 4,
                })
                logger.info("üìä Mid-range GPU optimization enabled")
                
            else:
                # D√º≈ü√ºk VRAM GPU'lar
                params.update({
                    "n_ctx": 2048,
                    "n_batch": 512,
                    "n_threads": 4,
                    "n_gpu_layers": 10,     # Az layer GPU'da
                    "low_vram": True,
                    "f16_kv": True,
                    "logits_all": False,
                    "n_threads_batch": 2,
                })
                logger.info("‚ö° Low VRAM GPU optimization enabled")
        else:
            # CPU-only mod
            params.update({
                "n_ctx": 2048,
                "n_batch": 512,
                "n_threads": min(8, os.cpu_count()),
                "n_gpu_layers": 0,
                "n_threads_batch": min(4, os.cpu_count()),
            })
            logger.info("üíª CPU-only mode enabled")
        
        return params
    
    def generate_response(self, query: str, context: str, chat_history: Optional[List] = None) -> Tuple[str, float]:
        """
        Response olu≈ütur
        
        Returns:
            Tuple[str, float]: (response, duration_seconds)
        """
        start_time = datetime.now()
        
        if not self.llm:
            fallback_response = self._generate_fallback_response(query, context)
            duration = (datetime.now() - start_time).total_seconds()
            return fallback_response, duration
        
        try:
            # Prompt olu≈ütur
            prompt = self._create_prompt(query, context, chat_history)
            
            # A100 i√ßin optimize edilmi≈ü generation parametreleri
            generation_params = self._get_generation_params()
            
            logger.debug(f"üî§ Prompt length: {len(prompt)} chars")
            
            # Response olu≈ütur
            response = self.llm(
                prompt,
                **generation_params
            )
            
            # Response'u temizle
            response_text = self._extract_response_text(response)
            response_text = self._clean_response(response_text)
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"‚úÖ GGUF response generated in {duration:.2f}s")
            
            return response_text, duration
            
        except Exception as e:
            logger.error(f"‚ùå GGUF generation failed: {e}")
            fallback_response = self._generate_fallback_response(query, context)
            duration = (datetime.now() - start_time).total_seconds()
            return fallback_response, duration
    
    def _create_prompt(self, query: str, context: str, chat_history: Optional[List] = None) -> str:
        """T√ºrk√ße finansal analiz i√ßin prompt olu≈ütur"""
        
        system_prompt = """Sen finansal dok√ºmanlarƒ± analiz eden uzman bir asistansƒ±n. Verilen baƒülam bilgilerini kullanarak T√ºrk√ße cevap ver.

Kurallar:
- Sadece baƒülam bilgilerini kullan
- Eƒüer baƒülamda cevap yoksa "Bu bilgi dok√ºmanlarda bulunmuyor" de
- Finansal terimleri doƒüru kullan
- Sayƒ±sal verileri dikkatli kontrol et
- Kƒ±sa ve net cevaplar ver
- Kaynaklarƒ± belirt"""

        # Mistral chat format
        if "mistral" in self.model_name.lower():
            prompt = f"<s>[INST] {system_prompt}\n\nBaƒülam Bilgileri:\n{context}\n\nSoru: {query} [/INST]"
        else:
            # Genel format
            prompt = f"System: {system_prompt}\n\nContext: {context}\n\nUser: {query}\nAssistant:"
        
        return prompt
    
    def _get_generation_params(self) -> Dict:
        """Generation parametrelerini d√∂nd√ºr"""
        return {
            "max_tokens": 1024,
            "temperature": 0.7,
            "top_p": 0.95,
            "top_k": 40,
            "repeat_penalty": 1.1,
            "stop": ["</s>", "[INST]", "[/INST]", "System:", "User:"],
            "stream": False,
            "echo": False,
        }
    
    def _extract_response_text(self, response) -> str:
        """Response'dan metni √ßƒ±kar"""
        if isinstance(response, dict):
            if 'choices' in response:
                return response['choices'][0]['text'].strip()
            elif 'content' in response:
                return response['content'].strip()
        
        return str(response).strip()
    
    def _clean_response(self, response_text: str) -> str:
        """Response'u temizle"""
        # Unwanted token'larƒ± kaldƒ±r
        unwanted_tokens = ["</s>", "[INST]", "[/INST]", "<s>", "System:", "User:", "Assistant:"]
        for token in unwanted_tokens:
            response_text = response_text.replace(token, "")
        
        # Fazla bo≈üluklarƒ± temizle
        response_text = " ".join(response_text.split())
        
        # Tekrar eden satƒ±rlarƒ± kaldƒ±r
        lines = response_text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and line not in cleaned_lines[-3:]:  # Son 3 satƒ±rda tekrar varsa atla
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()
    
    def _generate_fallback_response(self, query: str, context: str) -> str:
        """GGUF model √ßalƒ±≈ümadƒ±ƒüƒ±nda fallback response"""
        if context:
            return f"""
**[GGUF Model Hatasƒ±]**

Sorunuz: {query}

Bulunan ƒ∞lgili Bilgiler:
{context[:500]}...

üí° GGUF model d√ºzg√ºn y√ºklendiƒüinde bu bilgileri analiz ederek detaylƒ± cevap verebilirim.
"""
        else:
            return "Bu konuda dok√ºmanlarda ilgili bilgi bulunamadƒ±."
    
    def get_model_info(self) -> Dict:
        """Model bilgilerini d√∂nd√ºr"""
        return {
            "model_name": self.model_name,
            "model_loaded": self.llm is not None,
            "device": "gpu" if self.llm else "unknown",
            "model_path": self.model_path,
            "service_type": "gguf"
        }

class HuggingFaceModelService:
    """
    HuggingFace modelleri i√ßin servis sƒ±nƒ±fƒ±
    """
    
    def __init__(self, model_id: str):
        """
        Args:
            model_id: HuggingFace model ID
        """
        self.model_id = model_id
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self._initialize_model()
    
    def _initialize_model(self):
        """HuggingFace modelini y√ºkle"""
        try:
            logger.info(f"üöÄ HuggingFace model y√ºkleniyor: {self.model_id}")
            
            # Device se√ßimi
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # Model ve tokenizer y√ºkle
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            
            # Pipeline olu≈ütur
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device_map="auto" if device == "cuda" else None
            )
            
            logger.info(f"‚úÖ HuggingFace model y√ºklendi - {device}")
            
        except Exception as e:
            logger.error(f"‚ùå HuggingFace model y√ºklenemedi: {e}")
    
    def generate_response(self, query: str, context: str, chat_history: Optional[List] = None) -> Tuple[str, float]:
        """Response olu≈ütur"""
        start_time = datetime.now()
        
        if not self.pipeline:
            fallback_response = "HuggingFace model y√ºklenmedi."
            duration = (datetime.now() - start_time).total_seconds()
            return fallback_response, duration
        
        try:
            # Prompt olu≈ütur
            prompt = self._create_prompt(query, context)
            
            # Generate
            outputs = self.pipeline(
                prompt,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # Response √ßƒ±kar
            response_text = outputs[0]['generated_text'][len(prompt):].strip()
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"‚úÖ HuggingFace response generated in {duration:.2f}s")
            
            return response_text, duration
            
        except Exception as e:
            logger.error(f"‚ùå HuggingFace generation failed: {e}")
            fallback_response = f"Generation hatasƒ±: {str(e)}"
            duration = (datetime.now() - start_time).total_seconds()
            return fallback_response, duration
    
    def _create_prompt(self, query: str, context: str) -> str:
        """Prompt olu≈ütur"""
        return f"""Sen finansal dok√ºmanlarƒ± analiz eden bir asistansƒ±n.

Baƒülam: {context}

Soru: {query}

Cevap:"""
    
    def get_model_info(self) -> Dict:
        """Model bilgilerini d√∂nd√ºr"""
        return {
            "model_name": self.model_id,
            "model_loaded": self.model is not None,
            "device": "gpu" if torch.cuda.is_available() else "cpu",
            "service_type": "huggingface"
        }

def create_optimized_llm_service(model_choice: str, model_path: Optional[str] = None):
    """
    Optimize edilmi≈ü LLM servisi olu≈ütur
    
    Args:
        model_choice: Model se√ßimi
        model_path: GGUF model path'i (eƒüer gerekirse)
    """
    
    if model_choice == "mistral_7b_gguf" and model_path and GGUF_AVAILABLE:
        return GGUFModelService(model_path)
    
    elif model_choice == "llama_3_1_8b" and HF_AVAILABLE:
        return HuggingFaceModelService("meta-llama/Meta-Llama-3.1-8B-Instruct")
    
    elif model_choice == "custom_drive_model" and model_path:
        # Path'e g√∂re format belirle
        if model_path.endswith('.gguf') and GGUF_AVAILABLE:
            return GGUFModelService(model_path)
        elif HF_AVAILABLE:
            return HuggingFaceModelService(model_path)
    
    # Fallback
    logger.warning(f"‚ö†Ô∏è Model olu≈üturulamadƒ±: {model_choice}")
    return None 